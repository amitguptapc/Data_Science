{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Word2Vec Model\n",
    "- Word2Vec Google's Pretrained Model\n",
    "- Contains vector representations of 50 billion words\n",
    "\n",
    "- Words which are similar in context have similar vectors\n",
    "- Distance/Similarity between two words can be measured using Cosine Distance\n",
    "\n",
    "### Applications\n",
    "- Text Similarity\n",
    "- Language Translation\n",
    "- Finding Odd Words\n",
    "- Word Analogies\n",
    "\n",
    "### Word Embeddings\n",
    "- Word embeddings are numerical representation of words, in the form of vectors.\n",
    "- Word2Vec Model represents each word as 300 Dimensional Vector\n",
    "- In this tutorial we are going to see how to use pre-trained word2vec model.\n",
    "- Model size is around 1.5 GB\n",
    "- We will work using Gensim, which is popular NLP Package.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Word2Vec Model\n",
    "**KeyedVectors** - This object essentially contains the mapping between words and embeddings. After training, it can be used directly to query those embeddings in various ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin',binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "v_apple = word_vectors[\"apple\"] \n",
    "v_india = word_vectors[\"india\"]\n",
    "print(v_apple.shape)\n",
    "print(v_india.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17158596]], dtype=float32)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([v_india],[v_apple])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Odd One Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accepts a list of words and returns the odd word\n",
    "def odd_one_out(words):\n",
    "    all_word_vectors = [word_vectors[w] for w in words]\n",
    "    avg_vector = np.mean(all_word_vectors,axis=0)\n",
    "    odd_one_out = None\n",
    "    min_similarity = 1.0\n",
    "    for w in words:\n",
    "        sim = cosine_similarity([word_vectors[w]],[avg_vector])\n",
    "        if sim < min_similarity:\n",
    "            min_similarity = sim\n",
    "            odd_one_out = w\n",
    "        print(\"Similarity b/w %s and average vector is %.2f\"%(w,sim))\n",
    "    return odd_one_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity b/w apple and average vector is 0.78\n",
      "Similarity b/w mango and average vector is 0.76\n",
      "Similarity b/w juice and average vector is 0.71\n",
      "Similarity b/w party and average vector is 0.36\n",
      "Similarity b/w orange and average vector is 0.65\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'party'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_1 = [\"apple\",\"mango\",\"juice\",\"party\",\"orange\"] \n",
    "odd_one_out(input_1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity b/w music and average vector is 0.66\n",
      "Similarity b/w dance and average vector is 0.81\n",
      "Similarity b/w sleep and average vector is 0.51\n",
      "Similarity b/w dancer and average vector is 0.72\n",
      "Similarity b/w food and average vector is 0.52\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sleep'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_2 = [\"music\",\"dance\",\"sleep\",\"dancer\",\"food\"]    \n",
    "odd_one_out(input_2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity b/w match and average vector is 0.58\n",
      "Similarity b/w player and average vector is 0.68\n",
      "Similarity b/w football and average vector is 0.72\n",
      "Similarity b/w cricket and average vector is 0.70\n",
      "Similarity b/w dancer and average vector is 0.53\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'dancer'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_3  = [\"match\",\"player\",\"football\",\"cricket\",\"dancer\"]\n",
    "odd_one_out(input_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity b/w india and average vector is 0.81\n",
      "Similarity b/w paris and average vector is 0.75\n",
      "Similarity b/w russia and average vector is 0.79\n",
      "Similarity b/w france and average vector is 0.81\n",
      "Similarity b/w germany and average vector is 0.84\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'paris'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_4 = [\"india\",\"paris\",\"russia\",\"france\",\"germany\"]\n",
    "odd_one_out(input_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Word Analogies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the word analogy task, we complete the sentence \"a is to b as c is to __\". An example is 'man is to woman as king is to queen' . In detail, we are trying to find a word d, such that the associated word vectors `ea,eb,ec,ed` are related in the following manner: `eb−ea ≈ ed−ec`. We will measure the similarity between `eb−ea` and `ed−ec` using cosine similarity. \n",
    "\n",
    "![Word2Vec](./img/word2vec.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Examples\n",
    "`man -> woman :: prince -> princess`  \n",
    "`italy -> italian :: spain -> spanish`  \n",
    "`india -> delhi :: japan -> tokyo`  \n",
    "`man -> woman :: boy -> girl`  \n",
    "`small -> smaller :: large -> larger`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accepts a triad of words, a,b,c and returns d, such that a : b :: c : d\n",
    "def predict_word(a,b,c,word_vectors):\n",
    "    a,b,c = a.lower(),b.lower(),c.lower()\n",
    "    max_similarity = -100 # similarity b/w |b-a| & |d-c| should be max\n",
    "    d = None\n",
    "    words = word_vectors.vocab.keys()\n",
    "    wa,wb,wc = word_vectors[a],word_vectors[b],word_vectors[c]\n",
    "    for w in words:\n",
    "        if w in [a,b,c]:\n",
    "            continue\n",
    "        wv = word_vectors[w]\n",
    "        sim = cosine_similarity([wb-wa],[wv-wc])\n",
    "        if sim > max_similarity: \n",
    "            max_similarity = sim\n",
    "            d = w            \n",
    "    return d    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triad = (\"man\",\"woman\",\"prince\")\n",
    "predict_word(*triad,word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the Most Similar Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118192911148071)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training Our Own Word2Vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec model can learn embeddings from any text corpus!\n",
    "- Continuous Bag of Words Model\n",
    "- Skip Gram Model\n",
    "\n",
    "Algorithm looks at window of target word(Y) to provide context word(X), the model is trained on (X,Y) pairs in a superwised manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Each sentence must be tokenized, into a list of words.\n",
    "\n",
    "- The sentences can be text loaded into memory once,\n",
    "or we can build a data pipeline which iteratively feeds data to the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopw  = set(stopwords.words('english'))\n",
    "def readFile(file): \n",
    "    f = open(file,'r',encoding='utf-8')\n",
    "    text = f.read()\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    data = []\n",
    "    for sent in sentences:\n",
    "        words =  nltk.word_tokenize(sent)\n",
    "        words = [w.lower() for w in words if len(w)>2 and w not in stopw]\n",
    "        data.append(words)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['deepika', 'padukone', 'ranveer', 'singh', 'wedding', 'one', 'biggest', 'bollywood', 'events', 'happened', '2018'], ['the', 'deepika', 'ranveer', 'celebrations', 'hooked', 'phones', 'waiting', 'come', 'also', 'gave', 'enough', 'reason', 'believe', 'stylish', 'two', 'couple'], ['from', 'airport', 'looks', 'reception', 'parties', 'everything', 'entire', 'timeline', 'deepika', 'ranveer', 'wedding', 'style', 'file'], ['not', 'ambanis', 'deepika', 'ranveer', 'priyanka', 'nick'], ['man', 'proves', 'wedding', 'the', 'year', 'this', 'year', 'year', 'big', 'fat', 'lavish', 'extravagant', 'weddings'], ['from', 'isha', 'ambani', 'anand', 'piramal', 'deepika', 'padukone', 'ranveer', 'singh', 'priyanka', 'chopra', 'nick', 'jonas', 'kapil', 'sharma', 'ginni', 'chatrath', '2018', 'saw', 'many', 'grand', 'weddings'], ['but', 'nothing', 'beats', 'man', 'wedding', 'the', 'year', 'award', 'social', 'media'], ['priyanka', 'also', 'shared', 'video', 'featuring', 'nick', 'jonaswas', 'also', 'celebrating', 'the', 'family', 'first', 'celebrated', 'christmas', 'london', 'pictures', 'priyanka', 'chopra', 'nick', 'jonas', 'new', 'year', 'celebrations', 'outstanding'], ['priyanka', 'chopra', 'nick', 'shared', 'glimpses', 'celebration', 'verbier', 'switzerland'], ['priyanka', 'chopra', 'married', 'nick', 'jonas', 'december', 'three', 'wedding', 'receptions', 'one', 'new', 'delhi', 'two', 'mumbai'], ['this', 'year', 'year', 'big', 'fat', 'lavish', 'extravagant', 'weddings'], ['from', 'isha', 'ambani', 'anand', 'piramal', 'deepika', 'padukone', 'ranveer', 'singh', 'priyanka', 'chopra', 'nick', 'jonas', 'kapil', 'sharma', 'ginni', 'chatrath', '2018', 'saw', 'many', 'grand', 'weddings'], ['but', 'nothing', 'beats', 'man', 'wedding', 'the', 'year', 'award', 'social', 'media'], ['kapil', 'sharma', 'ginni', 'chatrath', 'jaggo', 'night', 'december', 'made', 'even', 'special', 'industry', 'friends'], ['kapil', 'sharma', 'ginni', 'chatrath', 'friends', 'long', 'time'], ['there', 'virat', 'side', 'actress', 'wife', 'anushka', 'sharma', 'pleasure', 'audience'], ['while', 'couple', 'rang', 'new', 'year', 'style', 'morning', 'saw', 'virat', 'dress', 'squad', 'attire', 'anushka', 'pink', 'salwar', 'suit'], ['isha', 'ambani', 'married', 'anand', 'piramal', 'year']]\n"
     ]
    }
   ],
   "source": [
    "text = readFile('news.txt')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=116, size=300, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(text,size=300,window=10,min_count=1)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deepika', 'padukone', 'ranveer', 'singh', 'wedding', 'one', 'biggest', 'bollywood', 'events', 'happened', '2018', 'the', 'celebrations', 'hooked', 'phones', 'waiting', 'come', 'also', 'gave', 'enough', 'reason', 'believe', 'stylish', 'two', 'couple', 'from', 'airport', 'looks', 'reception', 'parties', 'everything', 'entire', 'timeline', 'style', 'file', 'not', 'ambanis', 'priyanka', 'nick', 'man', 'proves', 'year', 'this', 'big', 'fat', 'lavish', 'extravagant', 'weddings', 'isha', 'ambani', 'anand', 'piramal', 'chopra', 'jonas', 'kapil', 'sharma', 'ginni', 'chatrath', 'saw', 'many', 'grand', 'but', 'nothing', 'beats', 'award', 'social', 'media', 'shared', 'video', 'featuring', 'jonaswas', 'celebrating', 'family', 'first', 'celebrated', 'christmas', 'london', 'pictures', 'new', 'outstanding', 'glimpses', 'celebration', 'verbier', 'switzerland', 'married', 'december', 'three', 'receptions', 'delhi', 'mumbai', 'jaggo', 'night', 'made', 'even', 'special', 'industry', 'friends', 'long', 'time', 'there', 'virat', 'side', 'actress', 'wife', 'anushka', 'pleasure', 'audience', 'while', 'rang', 'morning', 'dress', 'squad', 'attire', 'pink', 'salwar', 'suit']\n"
     ]
    }
   ],
   "source": [
    "words = list(model.wv.vocab)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_actor(a,b,c,word_vectors):\n",
    "    # Accepts a triad of words, a,b,c and returns d, such that a : b :: c : d\n",
    "    a,b,c = a.lower(),b.lower(),c.lower()\n",
    "    max_similarity = -100 \n",
    "    d = None\n",
    "    words = [\"ranveer\",\"deepika\",\"padukone\",\"singh\",\"nick\",\"jonas\",\"chopra\",\"priyanka\",\"virat\",\"anushka\",\"ginni\"]\n",
    "    wa,wb,wc = word_vectors[a],word_vectors[b],word_vectors[c]\n",
    "    for w in words:\n",
    "        if w in [a,b,c]:\n",
    "            continue\n",
    "        wv = word_vectors[w]\n",
    "        sim = cosine_similarity([wb-wa],[wv-wc])\n",
    "        if sim > max_similarity:\n",
    "            max_similarity = sim\n",
    "            d = w\n",
    "    return d    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chopra'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triad = (\"nick\",\"priyanka\",\"virat\")\n",
    "predict_actor(*triad,model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nick'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triad = (\"ranveer\",\"deepika\",\"priyanka\")\n",
    "predict_actor(*triad,model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nick'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triad = (\"ranveer\",\"singh\",\"deepika\")\n",
    "predict_actor(*triad,model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'virat'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triad = (\"deepika\",\"padukone\",\"priyanka\")\n",
    "predict_actor(*triad,model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format(\"myModel.bin\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
